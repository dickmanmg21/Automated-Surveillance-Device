#pi_object_detection
 
# USAGE
# python pi_object_detection.py --prototxt MobileNetSSD_deploy.prototxt.txt --model MobileNetSSD_deploy.caffemodel

#Code adapted from Adrian Rosebrock
#Link to code https://www.pyimagesearch.com/2017/10/16/raspberry-pi-deep-learning-object-detection-with-opencv/
#Face recognition adapted from  https://github.com/ageitgey/face_recognition by enric1994
 
# import the necessary packages\
#from picamera.array import PiRGBArray
from picamera import PiCamera
from time import sleep
import RPi.GPIO as GPIO
from imutils.video import VideoStream
from imutils.video import FPS
from multiprocessing import Process
from multiprocessing import Queue
import numpy as np
import argparse
import imutils
import time
import cv2
 
# Initialize some variables
tripped = 0 #num of times pir tripped
number = 1
GPIO.setmode(GPIO.BCM)
GPIO.setup(17,GPIO.IN)
input = GPIO.input(17)
PeopleSeen = 0 #counter for people seen
prev_input = 1 #initialize a previous imput variable to 1 (assume PIR triggered last)
 
def classify_frame(net, inputQueue, outputQueue):
            # keep looping
            while True:
                        # check to see if there is a frame in our input queue
                        if not inputQueue.empty():
                                    # grab the frame from the input queue, resize it, and
                                    # construct a blob from it
                                    frame = inputQueue.get()
                                    frame = cv2.resize(frame, (300, 300))
                                    blob = cv2.dnn.blobFromImage(frame, 0.007843,
                                                (300, 300), 127.5)
 
                                    # set the blob as input to our deep learning object
                                    # detector and obtain the detections
                                    net.setInput(blob)
                                    detections = net.forward()
 
                                    # write the detections to the output queue
                                    outputQueue.put(detections)
 
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-p", "--prototxt", required=True,
            help="path to Caffe 'deploy' prototxt file")
ap.add_argument("-m", "--model", required=True,
            help="path to Caffe pre-trained model")
ap.add_argument("-c", "--confidence", type=float, default=0.2,
            help="minimum probability to filter weak detections")
args = vars(ap.parse_args())
 
# initialize the list of class labels MobileNet SSD was trained todetect, then generate a set of bounding box colors for each class
CLASSES = ["background", "aeroplane", "bicycle", "bird", "boat",
            "bottle", "bus", "car", "cat", "chair", "cow", "diningtable",
            "dog", "horse", "motorbike", "person", "pottedplant", "sheep",
            "sofa", "train", "tvmonitor"]
COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))
 
# load our serialized model from disk
print("[INFO] loading model...")
net = cv2.dnn.readNetFromCaffe(args["prototxt"], args["model"])
 
# initialize the input queue (frames), output queue (detections),and the list of actual detections returned by the child process
inputQueue = Queue(maxsize=1)
outputQueue = Queue(maxsize=1)
detections = None
 
# construct a child process *indepedent* from our main process ofexecution
print("[INFO] starting process...")
p = Process(target=classify_frame, args=(net, inputQueue,
            outputQueue,))
p.daemon = True
p.start()
 
# initialize the video stream, allow the cammera sensor to warmup,and initialize the FPS counter
print("[INFO] starting video stream...")
vs = VideoStream(usePiCamera=True).start()
time.sleep(2.0)
fps = FPS().start()
 
# loop over the frames from the video stream
while True:
            # grab the frame from the threaded video stream, resize it, andgrab its dimensions
            frame = vs.read()
            frame = imutils.resize(frame, width=400)
            (fH, fW) = frame.shape[:2]
 
            # if the input queue *is* empty, give the current frame to
            # classify
            if inputQueue.empty():
                        inputQueue.put(frame)
 
            # if the output queue *is not* empty, grab the detections
            if not outputQueue.empty():
                        detections = outputQueue.get()
 
            # check to see if our detectios are not None (and if so, we'lldraw the detections on the frame)
            if detections is not None:
                        # loop over the detections
                        for i in np.arange(0, detections.shape[2]):
                                    # extract the confidence (i.e., probability) associatedwith the prediction
                                    confidence = detections[0, 0, i, 2]
 
                                    # filter out weak detections by ensuring the `confidence` is greater than the minimum confidence
                                    if confidence < args["confidence"]:
                                                continue
 
                                    # otherwise, extract the index of the class label from the `detections`, then compute the (x, y)-coordinatesof the bounding box for the object
                                    idx = int(detections[0, 0, i, 1])
                                    dims = np.array([fW, fH, fW, fH])
                                    box = detections[0, 0, i, 3:7] * dims
                                    (startX, startY, endX, endY) = box.astype("int")
 
                                    # draw the prediction on the frame
                                    label = "{}: {:.2f}%".format(CLASSES[idx],
                                                confidence * 100)
                                    cv2.rectangle(frame, (startX, startY), (endX, endY),
                                                COLORS[idx], 2)
                                    y = startY - 15 if startY - 15 > 15 else startY + 15
                                    cv2.putText(frame, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)
 
                                    #data collection
                                    #take a reading           
                                    input = GPIO.input(17)
                                    #if the last reading was low and this one high, print
                                    if ((CLASSES[idx] == "person") and ((not prev_input) and input)):
                                                PeopleSeen=PeopleSeen+1
                                                cv2.imwrite("/home/pi/Desktop/Surveillance_photos/PicturesOfSurvelience/DataObjectPicture-" + str(PeopleSeen) + ".jpg", frame)
           
            # Find all the faces and face encodings in the current frame of video
            face_locations = face_recognition.face_locations("/home/pi/Desktop/Surveillance_photos/PicturesOfSurvelience/DataObjectPicture-" + str(PeopleSeen) + ".jpg")
            print("Found {} faces in image.".format(len(face_locations)))
            num_faces=num_faces+len(face_locations)
            face_encodings = face_recognition.face_encodings("/home/pi/Desktop/Surveillance_photos/PicturesOfSurvelience/DataObjectPicture-" + str(PeopleSeen) + ".jpg", face_locations)
            appendMe = '\nThe PIR has been triggered ' + str(counter) + ' times and seen ' + str(num_faces) + ' faces total\n'
            appendFile = open("/home/pi/Desktop/Surveillance_photos/Data.txt","a+")
            appendFile.write(appendMe)
 X              appendFile.close()
            
 
# Compare Loop over each face found in the frame to see if it's someone we know.
         
for face_encoding in face_encodings:
For x in range(0, PeopleSeen - 1)   
                        match = face_recognition.compare_faces(["/home/pi/Desktop/Surveillance_photos/PicturesOfSurvelience/DataObjectPicture-" + str(x) + ".jpg" , face_encoding)
                        if match==1:
                                    name=face_encoding
                        print("I see a match with Person # {}!".format(name))
                        else:
                                    name = "Unknown Person"
                                    
appendFile = open("/home/pi/Desktop/Surveillance_photos/DataObjectDetection.txt","a+")
appendFile.write(appendMe)
appendMe = '\nThe PIR has detected ' + str(PeopleSeen) + ' total\n' 
appendFile.write(appendMe)
appendFile.close()

("I see someone named {}!".format(name))
            print("I have seen {} faces total!".format(num_faces))
 
                        #update previous input 
                        prev_input=input
       
       
 
       # show the output frame
            cv2.imshow("Frame", frame)
            key = cv2.waitKey(1) & 0xFF
 
            # if the `q` key was pressed, break from the loop
            if key == ord("q"):
                        break
 
            # update the FPS counter
            fps.update()
 
# stop the timer and display FPS information
fps.stop()
print("[INFO] elapsed time: {:.2f}".format(fps.elapsed()))
print("[INFO] approx. FPS: {:.2f}".format(fps.fps()))
print("[INFO] Number of people seen: %(people)d" %{"people": PeopleSeen})
 
#reading/writing data to separate file
f=open("/home/pi/Desktop/Surveillance_photos/DataObjectDetection.txt", "r")
appendMe=’\n’
if f.mode == 'r':
            contents =f.read()
if "House 1" not in contents: 
appendMe = 'House 1\n’
elif "House 2" not in contents: 
appendMe = 'House 2\n’
else:
            appendMe = 'House 3\n’
f.close()
 
appendFile = open("/home/pi/Desktop/Surveillance_photos/DataObjectDetection.txt","a+")
appendFile.write(appendMe)
appendMe = '\nThe PIR has detected ' + str(PeopleSeen) + ' total\n' 
appendFile.write(appendMe)
appendFile.close()
 
# do a bit of cleanup
cv2.destroyAllWindows()
vs.stop()





#frequency plot

from pylab import *
from rtlsdr import *

sdr = RtlSdr()

# configure device
sdr.sample_rate = 2.4e6
sdr.center_freq = 314.5e6
sdr.gain = 4

samples = sdr.read_samples(256*512)
sdr.close()

# use matplotlib to estimate and plot the PSD
psd(samples, NFFT=1024, Fs=sdr.sample_rate/1e6, Fc=sdr.center_freq/1e6)
xlabel('Frequency (MHz)')
ylabel('Relative power (dB)')
plt.savefig('/home/pi/Desktop/Recorded_Data.png')
show()

from rtlsdr import RtlSdr

sdr = RtlSdr()

# configure device
sdr.sample_rate = 2.048e6  # Hz
sdr.center_freq = 70e6     # Hz
sdr.freq_correction = 60   # PPM
sdr.gain = 'auto'

print(sdr.read_samples(512))
